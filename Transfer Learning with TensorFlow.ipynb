{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow\n",
    "\n",
    "*Transfer learning* is the practice of starting with a network that has already been trained, and then applying that network to your own problem.\n",
    "\n",
    "Because neural networks can often take days or even weeks to train, transfer learning (i.e. starting with a network that somebody else has already spent a lot of time training) can greatly shorten training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "In order to complete this lab, install Python 3, tensorflow, numpy, scipy, matplotlib, and pillow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "Here, you're going to practice transfer learning with [AlexNet](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiG34CS7vHPAhVKl1QKHW2JAJkQFggcMAA&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&usg=AFQjCNFlGsSmTUkJw0gLJ0Ry4cm961B7WA&bvm=bv.136593572,d.cGw).\n",
    "\n",
    "AlexNet is a popular base network for transfer learning because its structure is relatively straightforward, it's not too big, and it performs well empirically.\n",
    "\n",
    "Here is a TensorFlow implementation of AlexNet (adapted from [Michael Guerhoy and Davi Frossard](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from numpy import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_x = zeros((1, 227,227,3)).astype(float32)\n",
    "train_y = zeros((1, 1000))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]\n",
    "\n",
    "net_data = load(\"bvlc_alexnet.npy\", encoding=\"latin1\").item()\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i%group==0\n",
    "    assert c_o%group==0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    \n",
    "    if group==1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups = tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(3, group, kernel)\n",
    "        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(3, output_groups)\n",
    "    return  tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "resized = tf.image.resize_images(x, 227, 227)\n",
    "\n",
    "def features():\n",
    "\n",
    "    #conv1\n",
    "    #conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "    k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "    conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "    conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "    conv1_in = conv(resized, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "    conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "    #lrn1\n",
    "    #lrn(2, 2e-05, 0.75, name='norm1')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                                      depth_radius=radius,\n",
    "                                                      alpha=alpha,\n",
    "                                                      beta=beta,\n",
    "                                                      bias=bias)\n",
    "\n",
    "    #maxpool1\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "    #conv2\n",
    "    #conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "    k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "    conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "    conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "\n",
    "    #lrn2\n",
    "    #lrn(2, 2e-05, 0.75, name='norm2')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                                      depth_radius=radius,\n",
    "                                                      alpha=alpha,\n",
    "                                                      beta=beta,\n",
    "                                                      bias=bias)\n",
    "\n",
    "    #maxpool2\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool2')                                                  \n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    #conv3\n",
    "    #conv(3, 3, 384, 1, 1, name='conv3')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "    conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "    conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "    conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "    #conv4\n",
    "    #conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "    conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "    conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "    conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "\n",
    "    #conv5\n",
    "    #conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "    k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "    conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "    conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "    #maxpool5\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    #fc6\n",
    "    #fc(4096, name='fc6')\n",
    "    fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "    fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "    fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "\n",
    "    #fc7\n",
    "    #fc(4096, name='fc7')\n",
    "    fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "    fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "    fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "    return fc7\n",
    "\n",
    "def logits():\n",
    "    #fc8\n",
    "    #fc(1000, relu=False, name='fc8')\n",
    "    fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "    fc8b = tf.Variable(net_data[\"fc8\"][1])\n",
    "    fc8 = tf.nn.xw_plus_b(features(), fc8W, fc8b)\n",
    "    return fc8\n",
    "\n",
    "def probabilities():\n",
    "    #prob\n",
    "    #softmax(name='prob'))\n",
    "    return tf.nn.softmax(logits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Inference\n",
    "\n",
    "![alt text](poodle.png \"Poodle\")\n",
    "![alt text](weasel.png \"Weasel\")\n",
    "\n",
    "To start, run a few ImageNet images through the network, and verify that the network classifies them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: You don't need to edit this code.\n",
    "\n",
    "from caffe_classes import class_names\n",
    "\n",
    "# Initialize the Model\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = (imread(\"poodle.png\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"weasel.png\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Inference\n",
    "![alt text](construction.jpg \"Construction Sign\")\n",
    "![alt text](stop.jpg \"Stop Sign\")\n",
    "\n",
    "Next, run two of the traffic sign images through the network, and see how well the classifier performs.\n",
    "\n",
    "You'll notice, however, that the AlexNet model expects a 227x227x3 pixel image, whereas the traffic sign images are 32x32x3 pixels.\n",
    "\n",
    "In order to feed our the traffic sign images into AlexNet, you'll need to resize the images to the dimensions that AlexNet expects.\n",
    "\n",
    "You could resize the images outside of this program, but that would make for a huge collection of images. Instead, use the `tf.images.resize_images()` method to resize the images within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe_classes import class_names\n",
    "\n",
    "# TODO: Update the xdim, x, and resized variables to accomodate 32x32x3 pixel images.\n",
    "\n",
    "\n",
    "# NOTE: You don't need to edit the code below.\n",
    "# Initialize the Model\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = (imread(\"construction.jpg\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"construction.jpg\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "The problem is that AlexNet was trained on the [ImageNet](http://www.image-net.org/) database, which has 1000 classes of images. You can see the classes in the `caffe_classes.py` file. None of those classes involves traffic signs.\n",
    "\n",
    "In order to successfully classify our traffic sign images, you need to remove the final, 1000-neuron classification layer and replace it with a new, 43-neuron classification layer.\n",
    "\n",
    "This is called feature extraction, because you're basically extracting the images features captured by the penultimate layer, and passing them to a new classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Redefine the logits() function to create a new fully-connected layer.\n",
    "\n",
    "\n",
    "# NOTE: You don't need to edit the code below.\n",
    "# Initialize the Model\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = (imread(\"construction.jpg\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"construction.jpg\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (inds[-1-i], output[input_im_ind, inds[-1-i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Feature Extractor\n",
    "The feature extractor you just created works, in the sense that data will flow through the network and result in predictions.\n",
    "\n",
    "But the predictions aren't accurate, because you haven't yet trained the new classification layer.\n",
    "\n",
    "In order to do that, you'll need to read in the training dataset and train the network with cross entropy.\n",
    "\n",
    "Notice that in the network definition (look in the `features()` function), all of the layers are set to `trainable=False`. This freezes the weights of those layers, so you keep the trained AlexNet features and only train the final classification layer. This also makes training faster.\n",
    "\n",
    "Training AlexNet (even just the final layer!) can take a little while, so it can be helpful to try out your code using only a small portion of the training set. Once you're confident your implementation works, you can train use the entire training dataset to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Load the training dataset.\n",
    "\n",
    "\n",
    "# TODO: Pre-process the input data.\n",
    "\n",
    "\n",
    "# TODO: Once you are confident that the training works, update the training set to use all of the data.\n",
    "\n",
    "\n",
    "# TODO: Train the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "You've trained AlexNet as a feature extractor!\n",
    "\n",
    "Don't be discouraged if your validation accuracy still isn't as high as you'd like.\n",
    "\n",
    "Coming up, you'll explore other networks to use for transfer learning, as well as approaches to improve accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
